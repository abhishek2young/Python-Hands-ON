{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction technique \n",
    "* In Dimensionality reduction technique we reduce the number of features to reduce the complexity of the model.\n",
    "1. Feature selection\n",
    "  1. Filter method (Before model training) \n",
    "  2. Wrapper method (During model training)\n",
    "  3. Embedded method (After model training)\n",
    "2. Feature Extraction\n",
    "  1. Linear discriminant method \n",
    "  2. Principle component analysis(PCA)\n",
    "* 90% we use PCA \n",
    "> Feature selection\n",
    "* In feature selection we select the subset of original dataset i.e we are selecting the best features and then we are training the model on it.But in this case we are losing the data which may have some significance in predicting Y.\n",
    "* In Feature selection we select the best features by their variance the more the variance the more they contribute in predicting Y.\n",
    "* But when we are selecting the best feature between the two features if the variance of the both features are same then we may face problem while selecting the best feature between the two features in such cases we may use PCA.\n",
    "> Principle component analysis(PCA)\n",
    "* It is one of the most popular feature extraction techniques.\n",
    "* It is a supervised machine learning algorithm which is used for dimensionality reduction techniques.\n",
    "* In PCA we convert high dimensional data to low dimensional data.\n",
    "* When we had 2 feature in the data and we had to select the best feature among them but if they had same variance then we had to face problems while selecting the best feature.\n",
    "* And by selecting the best feature we may also avoid the another feature which was also contributing in predicting Y , this was happening in Feature selection.\n",
    "* In PCA we combine these 2 features and create a single feature which is the combination of the two features.\n",
    "* So in this way we are performing dimensionality reduction techniques and also in some instances we may increase the accuracy of the model.\n",
    "* Suppose we have features X1,X2,X3 and X4 as independent variables and Y as Target variable.\n",
    "* In PCA we wil get the number of columns(PC1,PC2,PC3,PC4)(PC = Principal Component) equal to number of features(X1,X2,X3,X4).\n",
    "* PC1 wil be the extraction of X1,X2,X3,X4.\n",
    "* PC2 wil be the extraction of X1,X2,X3,X4.\n",
    "* Same for PC3 and PC4.\n",
    "* That is we are having essence of X1,X2,X3,X4 in each principal component.\n",
    "* As PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 and suppose we are training our model on PC1,PC2,PC3,PC4 and we are getting the accuracy of the model as 96%.\n",
    "* As PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 , by using only PC1,PC2 we can get the accuracy of the model close to 96%.\n",
    "* In this way by selecting only PC1,PC2 we are getting the accuracy of the model close to 96% , which we were getting by combination of PC1,PC2,PC3,PC4 . So by selecting only PC1,PC2 we are reducing the number of features , so we are converting high dimensional data to low dimensional data.\n",
    "> Difference between covariance and coefficient of correlation(R)\n",
    "* R has a range of -1 to +1 , Covariance range is -infinity to +infinity.\n",
    "* Covariance shows directional relationship between 2 variables and R shows strength of relationship between 2 variables.\n",
    "* Formula for Covariance is = summation((Xi - Xmean) (Yi - Ymean)) / N\n",
    "* Formula for R = summation((Xi - Xmean)^2 (Yi - Ymean)^2) / sqrt((Xi - Xmean)^2 (Yi - Ymean)^2)\n",
    "* Formula for R = Covariance / Product of Standard Deviation\n",
    "> Mean \n",
    "* Mean tells us the central tendency of the data.\n",
    "> Variance \n",
    "* Formula for Variance = summation((Xi - Xmean)^2) / N\n",
    "* Variance tells us the spread of the data.\n",
    "* \n",
    "> Working of PCA \n",
    "* Previously when we had 2 features in the dataset with same variance we were not able to select the best feature among the two.\n",
    "* In PCA we are shifting the new X axis towards the data points and creating the difference between the variance.\n",
    "* While shifting the new X axis we will see for where we get the maximum variance after that the perpendicular line will be the Y axis.\n",
    "* The new X axis will PC1 and the new Y axis will PC2.\n",
    "> Difference between Variance and CoVariance \n",
    "* Variance will never give the direction , it will show the spread of the data.\n",
    "* Covariance will show the directional relationship between 2 variables.\n",
    "* From further on we will not use variance and the reason is that Covariance gives direction and spread of the data points and variance doesnot.\n",
    "> Revise on Eigen values and Eigen Vectors these both come under linear transformation.\n",
    "* In PCA Eigen values represent the amount of variance captured by each principal component.\n",
    "* Larger Eigen values indicate more significant components.\n",
    "* Eigen vectors represent the direction of maximum variance captured by each principal component.\n",
    "* In short Eigen values guide the importance of componenets and eigen vectors define their directions for dimensionality reduction in PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction techniques\n",
    "* In Dimensionality reduction techniques we reduce the number of features to reduce the complexity of the data.\n",
    "1. Feature selection\n",
    "  1. Filter method (Before model training)\n",
    "  2. Wrapper method (During model training)\n",
    "  3. Embedded method (After model training)\n",
    "2. Feature Extraction \n",
    "  1. Linear discriminant method \n",
    "  2. Principal component analysis(PCA)\n",
    "* 90% we use PCA \n",
    "> Feature Selection\n",
    "* In Feature Selection we select the subset of the original dataset i.e we are selecting the best features and then we will train our model on the selected features.\n",
    "* In Feature Selection we select the best features based on their variance the more the variance the mire it is contributing in predcting Y.\n",
    "* But when we are selecting the best features if we have 2 features and they have same variance then we may find problem while selecting the best feature while working in Feature Selection so we use PCA in such cases.\n",
    "> Principle component analysis(PCA)\n",
    "* It is one of the most popular Feature Extraction techniques.\n",
    "* It is a supervised machine learning algorithm which is used for dimensionality reduction techniques.\n",
    "* In PCA we convert high dimensional data to low dimensional data.\n",
    "* When we had 2 features having same variance in the dataset but when we had to select the best feature we had to face difficulty.\n",
    "* And by selecting the best feature we may also avoid the another feature which was contributing in predicting Y.\n",
    "* In this form we are loosing our data which is not a ideal way while building the model.\n",
    "* In PCA we combine both features and create a single feature which is combination of both features.\n",
    "* As we are reducing the number of features we are performing dimensionality reduction techniques and also we can increase the accuracy of the model.\n",
    "* Suppose we have X1,X2,X3,X4 as independent variables and Y as Target variable.\n",
    "* In PCA we will number of columns(PC1,PC2,PC3,PC4)(PC = Principal Component) equal to number of features(X1,X2,X3,X4).\n",
    "* PC1 will be the extraction of X1,X2,X3,X4 features.\n",
    "* PC2 will be the extraction of X1,X2,X3,X4 features.\n",
    "* Same for PC3 and PC4.\n",
    "* That is we are having essence of X1,X2,X3,X4 features in each Principal Component.\n",
    "* PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 features.Suppose we train our model on PC1,PC2,PC3,PC4 and we get the accuracy 96%. Using Only PC1 and PC2 we may get the accuracy close to 96% as each principal component is the extraction of X1,X2,X3,X4 features.\n",
    "* In this way by selecting the PC1 and PC2 we are getting accuracy equivalent to combination of PC1,PC2,PC3 and PC4 and also we are reducing the number of features and so we are converting high dimensional data to low dimensional data.\n",
    "> Difference between Covariance and Coefficient of Corelation\n",
    "* R has a range between -1 to +1 , Covariance has a range between -infinity to +infinity.\n",
    "* Covariance shows directional relationship between 2 variables , and R shows strength of relationship between 2 variables.\n",
    "* Formula for Covariance = summation((Xi - Xmean) (Yi - Ymean)) / N\n",
    "* Formula for R = summation((Xi - Xmean)^2 (Yi - Ymean)^2) / sqrt((Xi - Xmean^2) (Yi - Ymean^2))\n",
    "* Formula for R = Covariance / Product of standard deviation\n",
    "> Mean \n",
    "* Mean tells us the Central tendency of any kind of data.\n",
    "> Variance \n",
    "* Formula for variance = summation((Xi - Xmean)^2) / N\n",
    "* Variance tells us the spread of the data\n",
    "> Working of PCA \n",
    "* Previously when we had 2 features in the dataset with same variance we were not able to select the best feature among two.\n",
    "* In PCA we are shifting the new X axis towards the data points and creating the difference between the variance.\n",
    "* While shifting the new X axis we will see where we get the maximum variance after that the perpendicular line drawn will be the Y axis.\n",
    "* The New X axis will be PC1 and Y axis will be PC2.\n",
    "> Difference between Variance and Covariance\n",
    "* Variance will never give the direction , it will only show the spread of the data.\n",
    "* Covariance gives directional relationship between 2 data points.\n",
    "* From further on we will not use Variance and the reason is that Covariance gives direction and spread of the data points and Variance doesnot.\n",
    "> Revision on Eigen values and Eigen Vectors these both come under Linear Transformations\n",
    "* In PCA Eigen values represent the amount of variance captured by the Principal Component.\n",
    "* Larger Eigen values indicate more significant components.\n",
    "* Eigen Vectors represent the direction of maximum variance captured for each principal component.\n",
    "* In Short Eigen values guide the importance of components and Eigen Vectors represent define their directions for dimensionality reduction in PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis \n",
    "* In Principal Component Analysis we convert high dimensional dataset into low dimensional dataset.\n",
    "1. Collect the data \n",
    "2. We need to find out the Covariance matrix.Suppose we have X1,X2,X3,X4 as Independent variables.Then we will get 9 elements in Covariance matrix.By finding the Covariance we will find out the direction and spread of the data , in this matrix spread of the data is calculated by variance.\n",
    "* Formula for Covariance Con(X,Y) = summation((Xi - Xmean) (Yi - Ymean)) / N \n",
    "* After calculating Covariance matrix we will find out that we are getting variance in diagonal matrix and Covariance in Non Diagonal elements.\n",
    "* In Covariance matrix we get Variance(Spread of the data) as well as Covariance(Direction).SO we will use Covariance Matrix.\n",
    "> In PCA we wneed to perform Standardization on the data before going to PCA.\n",
    "> Before performing PCA we can also centralize the data(Mean of the data)\n",
    "3. Eigen Decomposition\n",
    "   1. Eigen Values \n",
    "   * Eigen Values = | A - Lambda*I| where A is the matrix , Lambda is the Scalar value and I is the Identity matrix.\n",
    "   * After solving above equation we will get 2 lambda values i.e we will get 2 Eigen Values.\n",
    "   * For 2*2 matrix we will get 2 Eigen Values and for 3*3 matrix we will get 3 Eigen Values.\n",
    "   2. Eigen Vectors \n",
    "   * EIgen Vectors = (A - Lambda*I) * X = 0 \n",
    "   * We will get 2 EIgen vector for 2 Eigen Values.\n",
    "* When we calculate Eigen Values for 2*2 Matrix we will get 2 Eigen Values.Among these 2 Eigen Values the max value among 2 Eigen Values will be selected and it Corresponding Eigen vector will be selected as PC1 (Principal Component1).\n",
    "* We are using Eigen Values and Eigen Vectors concept to find out which unit vector explains maximum variance.\n",
    "* Eigen Values and Eigen Vectors come under Linear Transformation.\n",
    "> Linear Transformation\n",
    "* iN linear transformation we are transforming X,Y coordinates i.e we are shifting the axis or rotating the axis.\n",
    "* In Linear Transformation our magnitude and direction of data point may change.\n",
    "* In some cases our direction will remain same but our magnitude may change these are called Eigen Vectors.\n",
    "* Eigen values will tell us how much our magnitude of EIgen vector has changed after the transformation.\n",
    "> Largest Eigen Vector of Covariance Matrix always points out in the direction of largest variance of the data.\n",
    "> Suppose we have 1000 data points and 3 columns\n",
    "* So we have 3 dimensional data we are performing PCA so we will perform dimensionality reduction techniques , so we will convert the 3 dimensional data into a 2 dimensional data or 1 dimensional data.\n",
    "* In PCA we will calculate the Covariance matrix and Covariance matrix will give direction as well as the spread of the data.\n",
    "3. Eigen Decomposition\n",
    "* In Eigen decomposition we will get 3 Eigen values or 3 Lambda values for 3*3 matrix.\n",
    "* Then we will calculate the Eigen vector and we will get 3 Eigen vectors.\n",
    "* AX = Lambda * X , we can also write it as AX - Lambda * X = 0 , Or [A - Lambda * I]* X = 0\n",
    "* In Above formula we have X as Eigen vector and Lambda as Eigen Values.\n",
    "4. Tranformation \n",
    "* In Tranformation we will get 3 Principal Components in a DataFrame.\n",
    "* PC1 = 1000 * 1 , PC2 = 1000 * 1 , PC3 = 1000 * 1\n",
    "> SUmmary \n",
    "* We need to find the feature of maximum variance in Feature Extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "\n",
       "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3        0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "   pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0        0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1        0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2        0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3        0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4        0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "\n",
       "   pixel_7_7  Target  \n",
       "0        0.0       0  \n",
       "1        0.0       1  \n",
       "2        0.0       2  \n",
       "3        0.0       3  \n",
       "4        0.0       4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(digit_data.data , columns=digit_data.feature_names)\n",
    "df[\"Target\"] = digit_data.target\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  3., 15., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  7., 15., 16., 16.,  2.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_data.images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c92e9e3290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(digit_data.images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(digit_data.images[1])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(digit_data.target[i])\n",
    "    plt.matshow(digit_data.images[i])\n",
    "    plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"Target\",axis=1)\n",
    "y = df[\"Target\"]\n",
    "x_train , x_test , y_train , y_test = train_test_split(x, y , test_size=0.20 , random_state=42 , stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "std_scalar = StandardScaler()\n",
    "std_array = std_scalar.fit_transform(x_train)\n",
    "\n",
    "x_train_std = pd.DataFrame(std_array , columns=x_train.columns)\n",
    "x_train_std = x_train_std.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "x_train_std = np.ascontiguousarray(x_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9839944328462074"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(x_train_std , y_train)\n",
    "knn_model.score(x_train_std , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With PCA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1437 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "421         0.0        0.0       11.0       10.0       12.0       14.0   \n",
       "825         0.0        0.0        2.0       13.0       15.0        7.0   \n",
       "885         0.0        1.0       15.0       16.0       16.0       16.0   \n",
       "182         0.0        0.0        5.0       16.0       12.0        2.0   \n",
       "151         0.0        0.0        0.0        0.0        5.0       14.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "471         0.0        0.0        0.0        6.0       16.0        6.0   \n",
       "1445        0.0        0.0        2.0       12.0        7.0        0.0   \n",
       "1412        0.0        0.0        1.0       11.0       12.0        9.0   \n",
       "559         0.0        0.0        4.0       10.0       15.0       16.0   \n",
       "813         0.0        0.0        3.0        4.0       10.0        0.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
       "421        11.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "825         1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "885         5.0        0.0        0.0        7.0  ...        0.0        0.0   \n",
       "182         0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "151         3.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "471         0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1445        0.0        0.0        0.0        0.0  ...        3.0        0.0   \n",
       "1412        5.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "559         4.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "813         0.0        0.0        0.0        3.0  ...       16.0        0.0   \n",
       "\n",
       "      pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
       "421         0.0        0.0        9.0       14.0        9.0        2.0   \n",
       "825         0.0        0.0        3.0       14.0       14.0        1.0   \n",
       "885         0.0        3.0       15.0       16.0        6.0        0.0   \n",
       "182         0.0        0.0        6.0       11.0        0.0        0.0   \n",
       "151         0.0        0.0        0.0        0.0        5.0       13.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "471         0.0        0.0        0.0        4.0       13.0       11.0   \n",
       "1445        0.0        0.0        1.0       13.0       15.0        9.0   \n",
       "1412        0.0        0.0        3.0       13.0       15.0        3.0   \n",
       "559         0.0        0.0        6.0       16.0        4.0        0.0   \n",
       "813         0.0        0.0        0.0        3.0       11.0       16.0   \n",
       "\n",
       "      pixel_7_6  pixel_7_7  \n",
       "421         0.0        0.0  \n",
       "825         0.0        0.0  \n",
       "885         0.0        0.0  \n",
       "182         0.0        0.0  \n",
       "151         4.0        0.0  \n",
       "...         ...        ...  \n",
       "471         2.0        0.0  \n",
       "1445        0.0        0.0  \n",
       "1412        0.0        0.0  \n",
       "559         0.0        0.0  \n",
       "813        16.0        3.0  \n",
       "\n",
       "[1437 rows x 64 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>1.237883</td>\n",
       "      <td>-0.427447</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>1.430144</td>\n",
       "      <td>2.841302</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>0.686967</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>-0.572426</td>\n",
       "      <td>-0.819734</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.670969</td>\n",
       "      <td>0.280748</td>\n",
       "      <td>0.720320</td>\n",
       "      <td>0.194886</td>\n",
       "      <td>-0.115216</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-0.499062</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>0.444104</td>\n",
       "      <td>-0.989273</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.753726</td>\n",
       "      <td>2.086262</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.952718</td>\n",
       "      <td>1.783075</td>\n",
       "      <td>1.067392</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>1.589622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>2.911853</td>\n",
       "      <td>1.872995</td>\n",
       "      <td>0.884756</td>\n",
       "      <td>-1.182344</td>\n",
       "      <td>-1.158812</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.034685</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>-0.687442</td>\n",
       "      <td>-0.410868</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>0.093952</td>\n",
       "      <td>-0.266869</td>\n",
       "      <td>-2.402180</td>\n",
       "      <td>-1.158812</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-1.095159</td>\n",
       "      <td>-2.788100</td>\n",
       "      <td>-1.603659</td>\n",
       "      <td>1.430144</td>\n",
       "      <td>0.476088</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-1.092076</td>\n",
       "      <td>-2.800446</td>\n",
       "      <td>-1.385650</td>\n",
       "      <td>1.045196</td>\n",
       "      <td>0.474237</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-1.095159</td>\n",
       "      <td>-1.371708</td>\n",
       "      <td>0.952718</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.410868</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-1.092076</td>\n",
       "      <td>-1.879145</td>\n",
       "      <td>0.240798</td>\n",
       "      <td>0.706118</td>\n",
       "      <td>-0.015155</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.670969</td>\n",
       "      <td>0.044683</td>\n",
       "      <td>-1.138863</td>\n",
       "      <td>-1.040373</td>\n",
       "      <td>-0.410868</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145909</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-0.894405</td>\n",
       "      <td>0.193781</td>\n",
       "      <td>0.647410</td>\n",
       "      <td>0.367040</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.883064</td>\n",
       "      <td>-0.191382</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.547817</td>\n",
       "      <td>1.067392</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-0.499062</td>\n",
       "      <td>0.193781</td>\n",
       "      <td>0.647410</td>\n",
       "      <td>-0.650195</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.246780</td>\n",
       "      <td>-0.427447</td>\n",
       "      <td>0.720320</td>\n",
       "      <td>1.783075</td>\n",
       "      <td>0.771740</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>-0.622609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>0.093952</td>\n",
       "      <td>0.884756</td>\n",
       "      <td>-1.588956</td>\n",
       "      <td>-1.158812</td>\n",
       "      <td>-0.504548</td>\n",
       "      <td>-0.193602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.330462</td>\n",
       "      <td>-0.458874</td>\n",
       "      <td>-1.843839</td>\n",
       "      <td>-0.441669</td>\n",
       "      <td>-1.040373</td>\n",
       "      <td>-0.410868</td>\n",
       "      <td>-0.125843</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>...</td>\n",
       "      <td>2.526367</td>\n",
       "      <td>-0.20077</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.295351</td>\n",
       "      <td>-1.092076</td>\n",
       "      <td>-2.109470</td>\n",
       "      <td>-0.165814</td>\n",
       "      <td>1.553813</td>\n",
       "      <td>3.410593</td>\n",
       "      <td>1.545184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1437 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0  -0.330462   1.237883  -0.427447   0.023127   1.430144   \n",
       "1           0.0  -0.330462  -0.670969   0.280748   0.720320   0.194886   \n",
       "2           0.0   0.753726   2.086262   0.988944   0.952718   1.783075   \n",
       "3           0.0  -0.330462  -0.034685   0.988944   0.023127  -0.687442   \n",
       "4           0.0  -0.330462  -1.095159  -2.788100  -1.603659   1.430144   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1432        0.0  -0.330462  -1.095159  -1.371708   0.952718   0.018420   \n",
       "1433        0.0  -0.330462  -0.670969   0.044683  -1.138863  -1.040373   \n",
       "1434        0.0  -0.330462  -0.883064  -0.191382   0.023127   0.547817   \n",
       "1435        0.0  -0.330462  -0.246780  -0.427447   0.720320   1.783075   \n",
       "1436        0.0  -0.330462  -0.458874  -1.843839  -0.441669  -1.040373   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
       "0      2.841302  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "1     -0.115216  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "2      1.067392  -0.125843  -0.061035   1.589622  ...  -0.762588   -0.20077   \n",
       "3     -0.410868  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "4      0.476088  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1432  -0.410868  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "1433  -0.410868  -0.125843  -0.061035  -0.622609  ...  -0.145909   -0.20077   \n",
       "1434   1.067392  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "1435   0.771740  -0.125843  -0.061035  -0.622609  ...  -0.762588   -0.20077   \n",
       "1436  -0.410868  -0.125843  -0.061035   0.325490  ...   2.526367   -0.20077   \n",
       "\n",
       "      pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
       "0     -0.026389  -0.295351   0.686967   0.424106  -0.572426  -0.819734   \n",
       "1     -0.026389  -0.295351  -0.499062   0.424106   0.444104  -0.989273   \n",
       "2     -0.026389   2.911853   1.872995   0.884756  -1.182344  -1.158812   \n",
       "3     -0.026389  -0.295351   0.093952  -0.266869  -2.402180  -1.158812   \n",
       "4     -0.026389  -0.295351  -1.092076  -2.800446  -1.385650   1.045196   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1432  -0.026389  -0.295351  -1.092076  -1.879145   0.240798   0.706118   \n",
       "1433  -0.026389  -0.295351  -0.894405   0.193781   0.647410   0.367040   \n",
       "1434  -0.026389  -0.295351  -0.499062   0.193781   0.647410  -0.650195   \n",
       "1435  -0.026389  -0.295351   0.093952   0.884756  -1.588956  -1.158812   \n",
       "1436  -0.026389  -0.295351  -1.092076  -2.109470  -0.165814   1.553813   \n",
       "\n",
       "      pixel_7_6  pixel_7_7  \n",
       "0     -0.504548  -0.193602  \n",
       "1     -0.504548  -0.193602  \n",
       "2     -0.504548  -0.193602  \n",
       "3     -0.504548  -0.193602  \n",
       "4      0.474237  -0.193602  \n",
       "...         ...        ...  \n",
       "1432  -0.015155  -0.193602  \n",
       "1433  -0.504548  -0.193602  \n",
       "1434  -0.504548  -0.193602  \n",
       "1435  -0.504548  -0.193602  \n",
       "1436   3.410593   1.545184  \n",
       "\n",
       "[1437 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing \n",
    "std_scalar = StandardScaler()\n",
    "std_array = std_scalar.fit_transform(x_train)\n",
    "x_train_std = pd.DataFrame(std_array , columns=x_train.columns)\n",
    "x_train_std = x_train_std.to_numpy()\n",
    "x_train_std = np.ascontiguousarray(x_train_std)\n",
    "x_train_std = pd.DataFrame(std_array , columns=x_train.columns)\n",
    "x_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.385000</td>\n",
       "      <td>1.778230</td>\n",
       "      <td>-2.121992</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>-1.509579</td>\n",
       "      <td>1.599376</td>\n",
       "      <td>0.535631</td>\n",
       "      <td>1.761216</td>\n",
       "      <td>-2.394267</td>\n",
       "      <td>-2.485801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291694</td>\n",
       "      <td>-0.124295</td>\n",
       "      <td>-0.115523</td>\n",
       "      <td>0.086217</td>\n",
       "      <td>-0.561383</td>\n",
       "      <td>0.634529</td>\n",
       "      <td>0.300475</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>6.349701e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.141795</td>\n",
       "      <td>-0.555521</td>\n",
       "      <td>-2.802031</td>\n",
       "      <td>1.000580</td>\n",
       "      <td>-1.108312</td>\n",
       "      <td>1.177479</td>\n",
       "      <td>1.269171</td>\n",
       "      <td>1.090178</td>\n",
       "      <td>-2.145553</td>\n",
       "      <td>-1.017084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.400524</td>\n",
       "      <td>0.256073</td>\n",
       "      <td>0.090896</td>\n",
       "      <td>-0.077804</td>\n",
       "      <td>-0.187201</td>\n",
       "      <td>0.033416</td>\n",
       "      <td>-2.132707e-16</td>\n",
       "      <td>-4.754565e-17</td>\n",
       "      <td>-5.494486e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.995326</td>\n",
       "      <td>2.283095</td>\n",
       "      <td>1.059102</td>\n",
       "      <td>5.750623</td>\n",
       "      <td>0.517032</td>\n",
       "      <td>-0.389269</td>\n",
       "      <td>-2.052327</td>\n",
       "      <td>0.189250</td>\n",
       "      <td>0.138376</td>\n",
       "      <td>-0.323042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848521</td>\n",
       "      <td>0.033847</td>\n",
       "      <td>0.093782</td>\n",
       "      <td>0.042591</td>\n",
       "      <td>-0.065460</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>0.281816</td>\n",
       "      <td>1.561805e-16</td>\n",
       "      <td>2.346466e-16</td>\n",
       "      <td>2.391246e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025393</td>\n",
       "      <td>2.990161</td>\n",
       "      <td>1.903785</td>\n",
       "      <td>-0.717131</td>\n",
       "      <td>-1.086422</td>\n",
       "      <td>-1.639004</td>\n",
       "      <td>2.590890</td>\n",
       "      <td>-0.812592</td>\n",
       "      <td>1.754160</td>\n",
       "      <td>1.818547</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116895</td>\n",
       "      <td>-0.073854</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>-0.341528</td>\n",
       "      <td>-0.215316</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>0.059053</td>\n",
       "      <td>3.125176e-16</td>\n",
       "      <td>-6.704689e-18</td>\n",
       "      <td>1.494958e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.607450</td>\n",
       "      <td>2.553288</td>\n",
       "      <td>-1.117519</td>\n",
       "      <td>-4.631118</td>\n",
       "      <td>-0.932052</td>\n",
       "      <td>0.596490</td>\n",
       "      <td>-2.239803</td>\n",
       "      <td>-0.027135</td>\n",
       "      <td>2.077975</td>\n",
       "      <td>-1.659698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212233</td>\n",
       "      <td>0.354552</td>\n",
       "      <td>-0.143587</td>\n",
       "      <td>-0.244427</td>\n",
       "      <td>0.027386</td>\n",
       "      <td>-0.176332</td>\n",
       "      <td>0.072950</td>\n",
       "      <td>-1.351092e-17</td>\n",
       "      <td>-7.285643e-17</td>\n",
       "      <td>2.002917e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2.201559</td>\n",
       "      <td>0.638386</td>\n",
       "      <td>3.132287</td>\n",
       "      <td>-2.494774</td>\n",
       "      <td>-1.489562</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>-2.984746</td>\n",
       "      <td>0.473825</td>\n",
       "      <td>-0.596160</td>\n",
       "      <td>0.507644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109266</td>\n",
       "      <td>-0.161554</td>\n",
       "      <td>-0.459627</td>\n",
       "      <td>-0.552496</td>\n",
       "      <td>0.291022</td>\n",
       "      <td>-0.060116</td>\n",
       "      <td>0.113979</td>\n",
       "      <td>-3.992922e-17</td>\n",
       "      <td>1.587375e-17</td>\n",
       "      <td>4.451458e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1.048280</td>\n",
       "      <td>-2.936994</td>\n",
       "      <td>-2.423821</td>\n",
       "      <td>1.295863</td>\n",
       "      <td>-0.206797</td>\n",
       "      <td>-0.756275</td>\n",
       "      <td>1.261480</td>\n",
       "      <td>1.092403</td>\n",
       "      <td>-0.623331</td>\n",
       "      <td>0.608937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194227</td>\n",
       "      <td>0.083233</td>\n",
       "      <td>0.341992</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>-0.067362</td>\n",
       "      <td>0.409367</td>\n",
       "      <td>-0.156643</td>\n",
       "      <td>7.859642e-17</td>\n",
       "      <td>1.574168e-18</td>\n",
       "      <td>1.903930e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>0.479065</td>\n",
       "      <td>0.727867</td>\n",
       "      <td>-3.618118</td>\n",
       "      <td>-0.048612</td>\n",
       "      <td>0.780131</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.395543</td>\n",
       "      <td>-0.437810</td>\n",
       "      <td>-1.057192</td>\n",
       "      <td>-1.234891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194537</td>\n",
       "      <td>0.225976</td>\n",
       "      <td>0.415111</td>\n",
       "      <td>-0.748867</td>\n",
       "      <td>-0.591911</td>\n",
       "      <td>-0.065626</td>\n",
       "      <td>-0.182608</td>\n",
       "      <td>6.006879e-17</td>\n",
       "      <td>-3.828670e-17</td>\n",
       "      <td>2.404219e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1.516755</td>\n",
       "      <td>3.232678</td>\n",
       "      <td>0.475808</td>\n",
       "      <td>0.666154</td>\n",
       "      <td>-1.012846</td>\n",
       "      <td>-1.891246</td>\n",
       "      <td>2.708977</td>\n",
       "      <td>-0.696377</td>\n",
       "      <td>1.377868</td>\n",
       "      <td>-0.246096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233902</td>\n",
       "      <td>-0.055572</td>\n",
       "      <td>0.184661</td>\n",
       "      <td>0.049171</td>\n",
       "      <td>0.231376</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>-0.215355</td>\n",
       "      <td>7.012130e-17</td>\n",
       "      <td>3.874764e-17</td>\n",
       "      <td>-8.806495e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>-1.616654</td>\n",
       "      <td>-2.312381</td>\n",
       "      <td>-0.860591</td>\n",
       "      <td>-2.989227</td>\n",
       "      <td>1.742698</td>\n",
       "      <td>-0.328946</td>\n",
       "      <td>-0.520159</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>2.120168</td>\n",
       "      <td>0.365380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542595</td>\n",
       "      <td>-0.174825</td>\n",
       "      <td>0.058259</td>\n",
       "      <td>0.489317</td>\n",
       "      <td>-0.017016</td>\n",
       "      <td>0.484349</td>\n",
       "      <td>0.115853</td>\n",
       "      <td>-1.093196e-16</td>\n",
       "      <td>-6.091156e-17</td>\n",
       "      <td>-2.250034e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1437 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.385000  1.778230 -2.121992  0.003672 -1.509579  1.599376  0.535631   \n",
       "1     3.141795 -0.555521 -2.802031  1.000580 -1.108312  1.177479  1.269171   \n",
       "2    -2.995326  2.283095  1.059102  5.750623  0.517032 -0.389269 -2.052327   \n",
       "3     0.025393  2.990161  1.903785 -0.717131 -1.086422 -1.639004  2.590890   \n",
       "4     2.607450  2.553288 -1.117519 -4.631118 -0.932052  0.596490 -2.239803   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1432  2.201559  0.638386  3.132287 -2.494774 -1.489562  0.005068 -2.984746   \n",
       "1433  1.048280 -2.936994 -2.423821  1.295863 -0.206797 -0.756275  1.261480   \n",
       "1434  0.479065  0.727867 -3.618118 -0.048612  0.780131  0.115132  0.395543   \n",
       "1435  1.516755  3.232678  0.475808  0.666154 -1.012846 -1.891246  2.708977   \n",
       "1436 -1.616654 -2.312381 -0.860591 -2.989227  1.742698 -0.328946 -0.520159   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0     1.761216 -2.394267 -2.485801  ...  0.291694 -0.124295 -0.115523   \n",
       "1     1.090178 -2.145553 -1.017084  ...  0.031112  0.400524  0.256073   \n",
       "2     0.189250  0.138376 -0.323042  ... -0.848521  0.033847  0.093782   \n",
       "3    -0.812592  1.754160  1.818547  ... -0.116895 -0.073854  0.009807   \n",
       "4    -0.027135  2.077975 -1.659698  ...  0.212233  0.354552 -0.143587   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1432  0.473825 -0.596160  0.507644  ...  0.109266 -0.161554 -0.459627   \n",
       "1433  1.092403 -0.623331  0.608937  ...  0.194227  0.083233  0.341992   \n",
       "1434 -0.437810 -1.057192 -1.234891  ...  0.194537  0.225976  0.415111   \n",
       "1435 -0.696377  1.377868 -0.246096  ...  0.233902 -0.055572  0.184661   \n",
       "1436  0.073164  2.120168  0.365380  ...  0.542595 -0.174825  0.058259   \n",
       "\n",
       "            57        58        59        60            61            62  \\\n",
       "0     0.086217 -0.561383  0.634529  0.300475 -0.000000e+00 -0.000000e+00   \n",
       "1     0.090896 -0.077804 -0.187201  0.033416 -2.132707e-16 -4.754565e-17   \n",
       "2     0.042591 -0.065460 -0.119720  0.281816  1.561805e-16  2.346466e-16   \n",
       "3    -0.341528 -0.215316  0.021943  0.059053  3.125176e-16 -6.704689e-18   \n",
       "4    -0.244427  0.027386 -0.176332  0.072950 -1.351092e-17 -7.285643e-17   \n",
       "...        ...       ...       ...       ...           ...           ...   \n",
       "1432 -0.552496  0.291022 -0.060116  0.113979 -3.992922e-17  1.587375e-17   \n",
       "1433  0.369082 -0.067362  0.409367 -0.156643  7.859642e-17  1.574168e-18   \n",
       "1434 -0.748867 -0.591911 -0.065626 -0.182608  6.006879e-17 -3.828670e-17   \n",
       "1435  0.049171  0.231376  0.003223 -0.215355  7.012130e-17  3.874764e-17   \n",
       "1436  0.489317 -0.017016  0.484349  0.115853 -1.093196e-16 -6.091156e-17   \n",
       "\n",
       "                63  \n",
       "0     6.349701e-15  \n",
       "1    -5.494486e-17  \n",
       "2     2.391246e-17  \n",
       "3     1.494958e-17  \n",
       "4     2.002917e-17  \n",
       "...            ...  \n",
       "1432  4.451458e-17  \n",
       "1433  1.903930e-18  \n",
       "1434  2.404219e-17  \n",
       "1435 -8.806495e-18  \n",
       "1436 -2.250034e-17  \n",
       "\n",
       "[1437 rows x 64 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "x_train_pca = pca.fit_transform(x_train_std)\n",
    "x_train_pca = pd.DataFrame(x_train_pca)\n",
    "x_train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(x_train_pca , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mknn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_pca\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    244\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fit_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mArgKminClassMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n",
      "File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_usable_for\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X, Y, metric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 471\u001b[0m         \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;66;03m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X)\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Y)\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;66;03m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    477\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    113\u001b[0m is_usable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    114\u001b[0m     get_config()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_cython_pairwise_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mis_numpy_c_ordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_metrics()\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_usable\n",
      "File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_numpy_c_ordered\u001b[39m(X):\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_contiguous\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "knn_model.score(x_train_pca , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis(PCA)\n",
    "* In Principal we can convert high dimensional dataset to low dimensional dataset.\n",
    "* Rules for PCA \n",
    "1. Collect the data \n",
    "2. We need to find out the Covariance matrix.Suppose we have X1,X2,X3 as independent variables.Then we will get 9 elements of the covariance matrix.By finding the covariance matrix we will get the direction nad spread of the data , in this matrix spread of the data is calculated by variance.\n",
    "* Formula for Covariance Cov(X,Y) = summation((Xi - Xmean) (Y - Ymean)) / N\n",
    "* After calculating the covariance matrix we will see that we are getting the variance in the diagonal matrix and covariance in non diagonal matrix.\n",
    "* In Covariance matrix we will get variance(spread of the data) as well as Covariance(Direction). SO we will use Covariance matrix.\n",
    "> In PCA when we collect the data , first of all we perform standardization of the data.\n",
    "> Before performing PCA we can also centralize the data(Mean of the data)\n",
    "3. Eigen Decomposition\n",
    "   1. Eigen Values \n",
    "   * Eigen Values = | A - Lambda*I| where A is the matrix , Lambda is the scalar value and I is the Identity matrix.\n",
    "   * After solving above equation we will get 2 lambda values i.e we will get 2 Eigen Values.\n",
    "   * For 2*2 matrix we will get 2 Eigen Values and for 3*3 matrix we will get 3 Eigen Values.\n",
    "   2. Eigen Vectors \n",
    "   * Eigen Vectors = (A - Lambda*I) * X = 0 \n",
    "   * We will get 2 Eigen vectors for 2 Eigen values.\n",
    "* When we calculate the Eigen values for 2*2 matrix we will get 2 Eigen Values. Among these 2 Eigen vlues the max value will be selected and its corresponding Eigen Vector will be selected as PC1(Principal Component 1).\n",
    "* We are using Eigen Values and Eigen Vectors concept to find out which unit vector explains Maximum Variance.\n",
    "* Eigen Values and Eigen Vectors come under Linear transformation.\n",
    "> Linear transformation\n",
    "* In Linear transformation we are transforming X,Y coordinates i.e we are shifting the axis or also we can say that we are rotating the axis.\n",
    "* In Linear transformation our magnitude and direction of data point may change.\n",
    "* In some cases our direction will remain the same but our magnitude may change these are called as Eigen vectors.\n",
    "* Eigen values will tell us how much our magnitude of Eigen vector has changed after the transformation.\n",
    "> Largest Eigen Vector of Covariance Matrix always points into the direction of largest variance of the data.\n",
    "> Suppose we have 1000 data points and 3 columns \n",
    "* So we have 3 dimension data , we are performing PCA so we are converting high dimensional dataset into Low dimensional dataset.So we will convert the 3 dimensional data into 2 dimensional dataset or 1 dimensional dataset.\n",
    "* In PCA we will calculate the covariance matrix , in Covariance Matrix we get the direction of the data as well as the spread of the data.\n",
    "3. Eigen decomposition\n",
    "* For above example we will get 3 lambda values for 3*3 matrix or we can say that we get 3 Eigen values.\n",
    "* Then we will get 3 Eigen Vectors for 3 Eigen Values.\n",
    "* AX = Lambda*X , we can also write it as AX - Lambda*X = 0 , [A - Lamda*I] * X  = 0 \n",
    "* In above formula we have X as Eigen Vectors and Lambda as Eigen Values.\n",
    "4. Transformation \n",
    "* In Transformation we will get 3 Principal components.\n",
    "* PC1 = 1000 * 1 , PC2 = 1000 * 1 , PC3 = 1000 * 1 \n",
    "> Summary \n",
    "* We need to find out the feature of maximum variance in Feature Extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
