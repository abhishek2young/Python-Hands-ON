{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is a Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes algorithm is a supervised machine learning algorithm, which is based on Bayes theorem an dused for classification problems.\n",
    "* It is mainly used in Text classification.\n",
    "* It performs well on High Dimensional training data set.(High Dimensional training data >> Number of rows is less than number of columns)\n",
    "* Naive Bayes clasasifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
    "* It is a probablistic classifier which means it predicts on the basis of the probability of an object.\n",
    "> Working \n",
    "* Working of Naive Bayes clasifier can be understood with the help of the below example.\n",
    "* Suppose we have a data set of weather conditions and corresponding target variable \"Play\". So using this dataset we need to decide that whether we should play or not on a particular day according to the weather conditions.So to solve this problem, we need to follow the below steps.\n",
    "1. Convert the given data set into frequency tables \n",
    "2. Generate likelihood table by finding the probablities of given features.\n",
    "3. Now use Bayes theorem to calculate the posterier probablity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What are the different types of Naive Bayes classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Types of Naive Bayes algorithms\n",
    "> Gaussian Naive Bayes \n",
    "* Continous data and normal distribution\n",
    "* If not then transform to normal \n",
    "> Bernoulli Naive Bayes\n",
    "* Binary BernoulliNB \n",
    "* Penalize the non occurence of a feature unlike Multinomial Naive Bayes.\n",
    "* Document Classification tasks\n",
    "> Multinomial Naive Bayes \n",
    "* Classifier uses the frequency/features of words for the predictors \n",
    "* Feature represent the count of unique words in the document.\n",
    "* If frequency 0 probability shows 0 hence it ignores that feature.\n",
    "* Text classification problems\n",
    "> There are 3 different types of Naive Bayes algorithms and which is used \n",
    "1. Gaussian Naive Bayes >> Continous and Normal distributed data (If data follows Gaussian distribution)\n",
    "2. Multinomial Naive Bayes >> Categorical and discrete data >> TF_IDF\n",
    "3. Bernoulli Naive Bayes >> All data is Binary (0 and 1) >> Bag of words\n",
    "> Details\n",
    "1. Gaussian Naive Bayes >> Assumes that features follow a Gaussian Distribution.It is suitable for cintinous features.\n",
    "2. Multi nomial Naive Bayes >> Primarily used for text classification with discrete features representing word counts or frequencies.\n",
    "3. Bernoullis Naive Bayes >> Suitable for Binary classification problems where features are binary varaiables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Naive Bayes is called Naive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is called Naive because it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you choose a classifier based on the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Small Training Set \n",
    "* Naive Bayes classifier tends to perform well even with a small amount of training data.They are simple ,computationally eficient and can handle limited data by making the assumption of feature independence.\n",
    "* Decision Tree > Decision Tree can work reasonably well with small datasets.They are non parameteric and can handle noisy data bu they might overfir if not pruined properly.\n",
    "* K-nearest Neighbors(KNN) > KNN is a lazy learning algorithm that doesn't explicitly build a model.It memorizes the entire training data set , so with smaller datasets it can be effective.\n",
    "\n",
    "> Medium to Large Training Set\n",
    "* Support Vector Machines (SVM) > SVM performs well on medium to large datasets.They can handle high dimensional data anfd find an optimal boundary between classes, but training time can increase with larger datasets.\n",
    "* Random Forest > Random Forest works well with large datasets . They are an ensemble of decision trees and tend to reduce Overfitting compared to individual decision trees.\n",
    "* Gradient Boosting(XGBoost,LightGBM) > These models perform well with large datasets and often outperform with other algorithms in various competitions.They build trees sequentially focusing on areas where previous trees performed poorly leading to better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Bayes theorem in detail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bayes theorem is a fundamental concept in probability theory and statistics that describe an probablity of an event ,based on a prior knowledge of conditions that might be relayed to an event.In simpler terms it helps uhs update our beleiefs about something (The event ) when we get new information(The evidence )\n",
    "* heres a breakdown of the theorem \n",
    "* Formula \n",
    "> P(A|B) = P(B|A) * P(A) / P(B)\n",
    "* where \n",
    "* P(A|B) is the Posterior probability of event A happening given that we know event B has already happened.\n",
    "* P(B|A) is the conditional probability of event B happening given that we know event A has already happened.\n",
    "* P(A) is the prior probability of event A happening.This is our initial belief about the event before we get any evidence.\n",
    "* P(B) is the marginal probability of event B happening . This is the probability of the evidence occurring regardless of whether event A happens or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the formula given by the Bayes theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Formula \n",
    "> P(A|B) = P(B|A) * P(A) / P(B)\n",
    "* Where \n",
    "* P(A|B) is the Posterior probability of event A happening given that we know event B has already happened.This is what we are trying to find.\n",
    "* P(B|A) is the conditional probability of event B happening given that we know event A has already happened.This is the likelihood of the evidence given the event.\n",
    "* P(A) is the prior probability of event A happening.This is our initial belief about the event before we get any evidence.\n",
    "* P(B) is the marginal probability of event B happening.This is the probability of the evidence occuring regardless of event A happens or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is posterior probability and prior probability in Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prior Probability \n",
    "* Represents the initial belief about the probability of an event occuring before any new information is available.\n",
    "> Posterior Probability \n",
    "* Represents the updated belief about the probablity of an event occuring after consideration new information.\n",
    "> Example \n",
    "> Prior Probability \n",
    "* Before examining the patient you have a prior belief about the probability of them having a certain disease based on their age,medical history and other factors.\n",
    "> Posterior Probability \n",
    "* After examining the patient and performing some tests you use the results to update your belief about the disease.This updated belief is the Posterior Probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define likelihood and evidence in Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Likelihood \n",
    "* definition > The probability of observing a specific set of features (Evidence) given that a particular class is TRUE.\n",
    "* Formula \n",
    "> P (F | C)  where \n",
    "* F represents the set of features\n",
    "* C represents a specific class \n",
    "* Evidence \n",
    "* Definition > A set of features observed in a particluar instance.\n",
    "* Representation > Often denoted by letter F in the formula\n",
    "> Example  \n",
    "* Evidence > The fingerprints found at the crime scene\n",
    "* Likkelihood > The probability that each suspects fingerprint matches the fingerprint found at the scene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bayes theorem in terms of prior,evidence and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prior Probability \n",
    "* Represents the initial belief about the probability of an event occurring before any new information is available.\n",
    "> Likelihood\n",
    "* The probability of observing a specific set of features (Evidence) given that a particular class is True.\n",
    "* Formula \n",
    "> P (F | C) where \n",
    "* F represents the set of features\n",
    "* C represents a specific class \n",
    "> Evidence \n",
    "* A set of features observed in a particular instance.\n",
    "* Often denoted by the letter F in the formula \n",
    "> Examples\n",
    "* Prior Probability > Before examining the patient you have a prior belief about the probability of them having a certain disease based on their age ,medical history and other factors.\n",
    "* Evidence > Fingerprints found at the crime scene\n",
    "* Likelihood > The probability that each suspects fingerprints matches the fingerprint found at the crime scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Naive Bayes Classifier works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bayes theorem is a fundamental concept in probability theory and statistics that describes the probability of an event based on prior knowledge of conditions that might be related to an event.\n",
    "1. Assumption of Feature Independence \n",
    "* The Naive Bayes algorithm assumes that the features used to describe the input data are conditionally independent given the class label. This a simplifying assumption and depsite its name it may not hold true in all real world scenarios. However this assumption makes the calculation tractable.\n",
    "2. Convert the dataset into a frequency table \n",
    "* In this first step dataset is converted into a frequency table.\n",
    "3. Create likelihood table by finding the probabilities \n",
    "4. Calculation of Posterior probabilities\n",
    "* When a new instance with features X needs to be classified the algorithm calculates the posterior probability for each class using Bayes theorem\n",
    "> P(A|B) = P(B|A) * P(A) / P(B)\n",
    "5. Classification\n",
    "* The algorithm assigns the class label that maximizes the Posterior probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While calculating the probability of a given situation what error can we run into in Naive Bayes and how can we solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While Naive Bayes is a powerful and efficient classification algorithm , it does have some limitations and potential problems that can affect its performance.Here are some common challenges and strategies to address these challenges.\n",
    "1. Assumption of Feature Independence \n",
    "> Issue \n",
    "* The assumption of feature independence may not hold in real life scenarios.If features are highly correlated the performance of the Naive Bayes algorithm will suffer.\n",
    "> Solution \n",
    "* Fetaure Engineering techniques or dimensionality reduction techniques can be applied to reduce the imoact of highly correlated features.Alternatively more sophisticated models that can capture dependencies beyween Features may be considered.\n",
    "2. Handling Zero Probabilities \n",
    "> Issue \n",
    "* If a feature in the test data set has not been seen during training, the likelihood for that feature becomes zero leading to a zero Posterior probability.\n",
    "> Solution \n",
    "* Use smoothing techniques such as Laplace smoothing (Additive smoothing) to assign a small probability to unseen features.This inolves adding a small constant to all feature counts during training.\n",
    "3. Numeric Underflow \n",
    "> Issue \n",
    "* When dealing with many small probablities , the product of these probablities can become very small leading to numerical underflow issues.\n",
    "> Solution \n",
    "* Use log probablities instead of raw probablities . Taking the algorithm of probablities turns products into sums which can help mitigate numerical underflow.\n",
    "4. Imbalanced class distribution \n",
    "> Issue \n",
    "* If one class is significantly more prelevant than others , the classifier may become biased towards that class.\n",
    "> Solution \n",
    "* Adjust the class priors to reflect the true class distribution.This involves estimating the prior probabilities.\n",
    "5. Continous Features \n",
    "> Issue \n",
    "* Naive Bayes assumes that features are categorical.If you have continous features you need to discretize them and the choice of discretization can impact performance.\n",
    "> Solution \n",
    "* Use techniques such as binning or kernel density estimation to discretize continous features.Alternatively consider using Gaussian Naive Bayes which assumes a Gauissian distribution for continous features.\n",
    "6. Model complexity \n",
    "> Issue \n",
    "* Naive Bayes is a simple model and may not capture the complex relationships in the data.\n",
    "> Solution \n",
    "* For complex relationships consider more sophisticated models.However keep in mind that more complex models require more data and compuataion \n",
    "7. Oultiers \n",
    "> Issue \n",
    "* Naive Bayes is sensitive to Outliers \n",
    "> Solution \n",
    "* Preprocess the data to handle Oultiers appropriately.Robust statistical measures or outlier removal Techniques may be aaplied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How would you use Naive Bayes classifier for Categorical features ?What if some features are numerical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Naive Bayes classifier can handle both Categorical and numerical features, making it a versatile tool for various classification tasks.\n",
    "Categorical Features\n",
    "* Encoding > Categorical Features need to be encoded numerically before being used by the model.One hot encoding is a common approach where each category is represented by new binary features.\n",
    "* Likelihood Calculation > The likelihood of a feature value occurring given a class is calculated as the frequency of that value within the class divided by the total number of instances belonging to the class.\n",
    "> Numerical Feature\n",
    "* No encoding needed > Numerical features can be directly used by the model without any transformation.\n",
    "* Likelihood Calculation > Different methods can be employed to estimate the likelihood of a numerical feature value give a class common approaches include \n",
    "* Gaussian distribution > Assuming the feature values follows a normal distribution within each class the likelihood is calculated using the corresponding mean and standard deviation.\n",
    "* Discretization > Dividing the feature range into bins and calculating the likelihood for each bin based on the frequency of values of each bin.\n",
    "* Kernel density estimation > estimating the probablity density function of the feature values within each class and using it to calculate the likelihood.\n",
    "> Handling Mixed features types \n",
    "1. Naive Bayes can easily handle datasets with both cateorical and numerical features.\n",
    "2. The model applies the appropriate method for likelihood calculation based on the data type of each feature.\n",
    "3. Its important to ensure consistency in the encoding and scaling of numerical features across the dataset to avoid bias towards certain features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whats the difference between Generative classifiers and Discriminative classifiers ? Name some examples of each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generative classifiers \n",
    "* Genreative models focuses on data distribution,offering data generation and insights into the data structure.\n",
    "* Focus > Modellling the underlying data distribution for each class.\n",
    "* Method > Estimates the joint probablity of features and class labels.This allows the model to generate new data points that resemble the learned distribution.\n",
    "* Examples > Naive Bayes ,Gaussian mixture models,Variational Autoencoders\n",
    "> Strengths \n",
    "* Can handle missing data by generating it based on the learned distribution.\n",
    "* Provide insights into the underlying data structure.\n",
    "* Useful for tasks like anomaly detection and data generation\n",
    "> Weakness \n",
    "* Usually computational expensive due to the need to model the entire data structure.\n",
    "* Prone to Overfitting,especially with limited data\n",
    "* May not be efficient for complex decision boundaries.\n",
    "\n",
    "> Dicriminative classifiers\n",
    "* Discriminative model focuses on the decision boundary , providing efficient classification with less computational cost.\n",
    "* Focus > Learning the decision boundary that separates different classes.\n",
    "* Method > Directly estimates the conditional probability of a class given a set of features.This directly addresses the classification problems.\n",
    "* Examples > Logistic regression , Support vector machines , Random Forests \n",
    "> Strengths \n",
    "* Faster training and predictions compared to generative models.\n",
    "* More efficent for complex decision boundaries and high dimensional data.\n",
    "* Less prone to Overfitting with limited data.\n",
    "> Weakness \n",
    "* Cannot generate new data points \n",
    "* Provide less information about the underlying data structure\n",
    "* May not be robust to missing data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Naive Bayes a discriminantive classifier or Generative Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes is a generative classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whether Feature scaling is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature scaling is generally not required for Naive Bayes , unlike some other machine learning algorithms like support vector machines or KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Outliers on NB Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outliers can have an impact on the performance of the Naive BAyes(NB) classifier as well as on other statistical and machine learning models.\n",
    "* Sensitivity to extreme outliers > Outliers or extreme values in one feature can influence the likelihood calculation for that feature.\n",
    "* Distortion of probability Estimates > Extreme values can pull the estimated mean and variance , affecting the shape of the distribution.\n",
    "* Overfitting > presence of outliers ,Naive BAyes may be sensitive to Overfitting ,especially if the dataset is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Bernoulli distribution in Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bernoulli distribution is a fundamental probability distribution used in the Naive Bayes classifier for binary features.It represents the probability of an event occuring only in two possible outcomes.\n",
    "* Key characteristics of the Bernoulli distribution \n",
    "* Discrete Distribution > Takes only two values 1 (Success) or 0 (Failure)\n",
    "* Single parameter > Defined by a single parameter p representing the probability of Success or Failure.\n",
    "* Probability mass Function (PMF)\n",
    "> P(X = 1) = p (Probability of Success)\n",
    "> P(X = 0) = 1 - p (Probability of Failure)\n",
    "* Benefits of using the Bernoulli distribution in the Naive Bayes classifier.\n",
    "1. Simpple and efficient > Easy to calculate the likelihood of binary features\n",
    "2. Interpretable > The parameter p directly represents the success probability , providing interpretability.\n",
    "3. Robustness > Less sensitive to Oulliers compared to other distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the advantages of Naive Bayes algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Naive Bayes is a simple and easy to understand algorithm.\n",
    "2. It requires minimal training time and memory compared to more complex algorithms.\n",
    "3. Naive Bayes performs well in high dimensional spaces making it particularly useful for text classification tasks with a large number of features.\n",
    "4. Naive Bayes can perform well with small datasets.\n",
    "5. Effective for text classification tasks and spam filteration \n",
    "6. Naive Bayes is relatively robust to irrelevant features.\n",
    "7. Naive Bayes requires minimal parameters tuning compared to more complex algorithms.\n",
    "8. Naive Bayes can handle both Categorical and numerical data making it versatile across different types of data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the disadvantages of Naive Bayes algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Naive Bayes assumes that features are conditionally independent given the class.This assumption may not hold true in many real worls scenarios.\n",
    "2. Presence of outliers or irrelevant features can significantly impact the models performance.\n",
    "3. Naive bayes can be prone to Overfitting especially when dealing with small data sets.\n",
    "4. Naive bayes may not perform well on Imbalanced datasets where the class distribution is skewed.\n",
    "5. Naive bayes can handle categorical features effectively, it requires additional steps to handle the continous categorical features.\n",
    "6. Naive bayes is not suited for problems where features have complex,non linear relationships.\n",
    "7. Naive bayes can somtimes be overly confident in its predictions even when the evidence is weak.\n",
    "8. Naive bayes is primarily intended for classification problems . It is not suitable for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the applications of Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentiment analysis > Classifying text as positive, negative or neutral.\n",
    "2. Spam filtering > Identifying spam emails based on their content.\n",
    "3. Document classification > Categorizing documents based on their content.\n",
    "4. News article Categorization > Classifying news articles based on their content.\n",
    "5. Object recognition > Recognizing objects based on their content.\n",
    "6. Product recommendation > Recommending products based on their past purchases and browsing histories..\n",
    "7. Movie recommendation > Recommending movies to users based on their ratings and viewing histories.\n",
    "8. Credit card fraud detection > Identifying fraudelent activities \n",
    "9. Telecom Fraud detection > Identifying fraudelent activities in telecommunication networks\n",
    "10. Customer churn detection > Predicting which customers are likely to stop the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
